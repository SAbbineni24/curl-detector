{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BICEP CURL DETECTOR**\n",
    "\n",
    "\n",
    "**Shankara Abbineni**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a personal project aimed at combining my interest in computer vision with my passion for exercise. A common problem faced by many individuals in the gym is forgetting to keep track of how many repetitions of an exercise have been completed. Often, it is more helpful to not focus on the number of repetitions but instead on the quality of the movement. As such, in order to solve this problem (to some extent), this project leverages the capabilities of OpenCV and MediaPipe to count the number of repetitions for bicep curls. In the future, I plan on expanding this basic structure to more exercises, including push-ups and pull-ups."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction, the packages we are using in this project are OpenCV for camera data, MediaPipe for pose estimation, and NumPy for calculating angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use OpenCV to pull a video feed and MediaPipe to extract landmarks. Then, using MediaPipe's drawing utilities, we can draw on the joints and connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils  # Getting all drawing utilites\n",
    "mp_pose = mp.solutions.pose  # Imports the pose estimation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Camera Feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Webcam Feed', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):  # Pressing 'q' stops the feed\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  # Close the OpenCV window\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Setting up a MediaPipe instance\n",
    "with mp_pose.Pose(min_tracking_confidence=0.5, min_detection_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Change cv BGR image to mp RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Making detections\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Turning RGB back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Rendering detections on frame\n",
    "        # Passing through image, landmarks from results, which landmark is connected to which, joint drawing spec, connection drawing spec\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(252, 80, 3), thickness=2, circle_radius=2), \n",
    "                                  mp_drawing.DrawingSpec(color=(3, 177, 252), thickness=2, circle_radius=2))\n",
    "\n",
    "        cv2.imshow('Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can extract information for the joints that matter in a bicep curl: the wrist, elbow, and shoulder. Let's focus on doing this for only the left arm and include the right arm later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Setting up a MediaPipe instance\n",
    "with mp_pose.Pose(min_tracking_confidence=0.5, min_detection_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Change cv BGR image to mp RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Making detections\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Turning RGB back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Extracting landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Rendering detections on frame\n",
    "        # Passing through image, landmarks from results, which landmark is connected to which, joint drawing spec, connection drawing spec\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(252, 80, 3), thickness=2, circle_radius=2), \n",
    "                                  mp_drawing.DrawingSpec(color=(3, 177, 252), thickness=2, circle_radius=2))\n",
    "\n",
    "        cv2.imshow('Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 0.7348299622535706\n",
       "y: 0.40620389580726624\n",
       "z: -0.46189093589782715\n",
       "visibility: 0.9992040991783142"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 0.9523245692253113\n",
       "y: 0.819460391998291\n",
       "z: -0.3104078471660614\n",
       "visibility: 0.9756779074668884"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 0.7348299622535706\n",
       "y: 0.40620389580726624\n",
       "z: -0.46189093589782715\n",
       "visibility: 0.9992040991783142"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angle Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are able to extract the coordinates of the specified joints, we can use the $\\tan$ function to find the angle of the arm at the elbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAngle(a, b, c):\n",
    "    a = np.array(a)  # First Joint\n",
    "    b = np.array(b)  # Middle Joint (main vertex)\n",
    "    c = np.array(c)  # Third Joint\n",
    "\n",
    "    radians = np.arctan2(a[1]-b[1], a[0]-b[0]) - np.arctan2(c[1]-b[1], c[0]-b[0])\n",
    "    degrees = np.abs(radians * 180.0 / np.pi)  # Coverting radians to degrees\n",
    "\n",
    "    if degrees > 180.0:\n",
    "        degrees = 360 - degrees\n",
    "\n",
    "    return degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159.73819208026083"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcAngle(left_wrist, left_elbow, left_shoulder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details about webcam resolution\n",
    "# A MacBook Air M1 has a webcam resolution of 1280x720\n",
    "\n",
    "webcam_width = 1280\n",
    "webcam_height = 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Setting up a MediaPipe instance\n",
    "with mp_pose.Pose(min_tracking_confidence=0.5, min_detection_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Change cv BGR image to mp RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Making detections\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Turning RGB back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Extracting landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Grab coordinates of left wrist, left elbow, and left shoulder\n",
    "            left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "\n",
    "            # Calculate angle of left arm\n",
    "            leftAngle = calcAngle(left_wrist, left_elbow, left_shoulder)\n",
    "\n",
    "            # Visualize angle on image\n",
    "            cv2.putText(image, str(leftAngle), \n",
    "                        tuple(np.multiply(left_elbow, [webcam_width, webcam_height]).astype(int)), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Rendering detections on frame\n",
    "        # Passing through image, landmarks from results, which landmark is connected to which, joint drawing spec, connection drawing spec\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(252, 80, 3), thickness=2, circle_radius=2), \n",
    "                                  mp_drawing.DrawingSpec(color=(3, 177, 252), thickness=2, circle_radius=2))\n",
    "\n",
    "        cv2.imshow('Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Curls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the pieces to detect and count bicep curls for one arm. We can first find the coordinates of the wrist, elbow, and shoulder. Then, we can calculate the angle at the elbow. We can set a certain threshold for when a curl is in the \"down\" phase and when it is in the \"up\" phase. The transition from \"down\" to \"up\" can be reflected in incrementing the counter by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Variables for counting curls\n",
    "left_counter = 0\n",
    "left_stage = None\n",
    "\n",
    "# Setting up a MediaPipe instance\n",
    "with mp_pose.Pose(min_tracking_confidence=0.5, min_detection_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Change cv BGR image to mp RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Making detections\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Turning RGB back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Extracting landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Grab coordinates of left wrist, left elbow, and left shoulder\n",
    "            left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "\n",
    "            # Calculate angle of left arm\n",
    "            leftAngle = calcAngle(left_wrist, left_elbow, left_shoulder)\n",
    "\n",
    "            # Visualize angle on image\n",
    "            cv2.putText(image, str(leftAngle), \n",
    "                        tuple(np.multiply(left_elbow, [webcam_width, webcam_height]).astype(int)), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Logic to know when to count a curl\n",
    "            if leftAngle > 160:\n",
    "                left_stage = 'down'\n",
    "            if leftAngle < 70 and left_stage == 'down':\n",
    "                left_counter += 1\n",
    "                left_stage = 'up'\n",
    "                print(left_counter)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Rendering the curl counter on the image feed\n",
    "        cv2.rectangle(image, (0, 0), (400, 100), (244, 177, 16), -1)  # Creating a box over the image\n",
    "\n",
    "        # Updating box with information about rep count\n",
    "        cv2.putText(image, 'Left Arm Reps', (15, 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(left_counter), (10, 80),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Updating box with information about rep stage\n",
    "        cv2.putText(image, 'Left Stage', (180, 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, left_stage, (170, 80),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Rendering detections on frame\n",
    "        # Passing through image, landmarks from results, which landmark is connected to which, joint drawing spec, connection drawing spec\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(252, 80, 3), thickness=2, circle_radius=2), \n",
    "                                  mp_drawing.DrawingSpec(color=(3, 177, 252), thickness=2, circle_radius=2))\n",
    "\n",
    "        cv2.imshow('Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Final Code To Both Arms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally extend the code to both arms, using information about the following joints: left wrist, left elbow, left shoulder, right wrist, right elbow, and right shoulder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam_width = 1280\n",
    "webcam_height = 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Variables for counting curls\n",
    "left_counter, right_counter = 0, 0\n",
    "left_stage, right_stage = None, None\n",
    "\n",
    "# Setting up a MediaPipe instance\n",
    "with mp_pose.Pose(min_tracking_confidence=0.5, min_detection_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Change cv BGR image to mp RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Making detections\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Turning RGB back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Extracting landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Grab coordinates of left wrist, left elbow, and left shoulder\n",
    "            left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "\n",
    "            # Grab coordinates of right wrist, right elbow, and right shoulder\n",
    "            right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "            right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "            right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "\n",
    "            # Calculate angle of left arm and right arm\n",
    "            leftAngle = calcAngle(left_wrist, left_elbow, left_shoulder)\n",
    "            rightAngle = calcAngle(right_wrist, right_elbow, right_shoulder)\n",
    "\n",
    "            # Visualize angles on image\n",
    "            cv2.putText(image, str(leftAngle), \n",
    "                        tuple(np.multiply(left_elbow, [webcam_width, webcam_height]).astype(int)), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(rightAngle), \n",
    "                        tuple(np.multiply(right_elbow, [webcam_width, webcam_height]).astype(int)), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Logic to know when to count a curl\n",
    "            if leftAngle > 160:\n",
    "                left_stage = 'down'\n",
    "            if leftAngle < 70 and left_stage == 'down':\n",
    "                left_counter += 1\n",
    "                left_stage = 'up'\n",
    "            if rightAngle > 160:\n",
    "                right_stage = 'down'\n",
    "            if rightAngle < 70 and right_stage == 'down':\n",
    "                right_counter += 1\n",
    "                right_stage = 'up'\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # Rendering the curl counter on the image feed for left arm\n",
    "        cv2.rectangle(image, (0, 0), (400, 100), (183, 159, 0), -1)  # Creating a box over the image\n",
    "\n",
    "        # Updating box with information about rep count\n",
    "        cv2.putText(image, 'Left Arm Reps', (15, 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(left_counter), (10, 80),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Updating box with information about rep stage\n",
    "        cv2.putText(image, 'Left Stage', (180, 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, left_stage, (170, 80),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        # Rendering the curl counter on the image feed for right arm\n",
    "        cv2.rectangle(image, (webcam_width-400, 0), (webcam_width, 100), (183, 159, 0), -1)  # Creating a box over the image\n",
    "\n",
    "        # Updating box with information about rep count\n",
    "        cv2.putText(image, 'Right Arm Reps', (15 + webcam_width - 400, 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(right_counter), (10 + webcam_width - 400, 80),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Updating box with information about rep stage\n",
    "        cv2.putText(image, 'Right Stage', (180 + webcam_width - 400, 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, right_stage, (170 + webcam_width - 400, 80),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Rendering detections on frame\n",
    "        # Passing through image, landmarks from results, which landmark is connected to which, joint drawing spec, connection drawing spec\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(73, 74, 254), thickness=2, circle_radius=2), \n",
    "                                  mp_drawing.DrawingSpec(color=(183, 159, 0), thickness=2, circle_radius=2))\n",
    "\n",
    "        cv2.imshow('Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
